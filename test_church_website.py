import requests
from bs4 import BeautifulSoup

def test_church_website():
    """êµíšŒ ì›¹ì‚¬ì´íŠ¸ ì ‘ì† í…ŒìŠ¤íŠ¸"""
    
    # ë‹¤ì–‘í•œ User-Agent ì‹œë„
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
        'Mozilla/5.0 (iPad; CPU OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
    ]
    
    url = "https://www.godswillseed.or.kr/bbs/board.php?bo_table=weekly"
    
    for i, user_agent in enumerate(user_agents):
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ {i+1}: {user_agent[:50]}...")
        
        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            print(f"  ìƒíƒœ ì½”ë“œ: {response.status_code}")
            print(f"  ì‘ë‹µ í¬ê¸°: {len(response.text)} bytes")
            
            # ì‘ë‹µ ë‚´ìš© í™•ì¸
            if "ìë™ë“±ë¡ë°©ì§€" in response.text or "ë³´ì•ˆì ˆì°¨" in response.text:
                print(f"  âŒ ë³´ì•ˆì ˆì°¨ ê°ì§€ë¨")
                print(f"  ì‘ë‹µ ë‚´ìš© ì¼ë¶€: {response.text[:200]}...")
            else:
                print(f"  âœ… ì •ìƒ ì ‘ì†ë¨")
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì£¼ë³´ ë§í¬ ì°¾ê¸°
                links = soup.find_all('a', href=True)
                weekly_links = [link for link in links if 'wr_id=' in link.get('href', '')]
                
                if weekly_links:
                    print(f"  ğŸ“‹ ì£¼ë³´ ë§í¬ {len(weekly_links)}ê°œ ë°œê²¬")
                    for j, link in enumerate(weekly_links[:3]):
                        href = link.get('href', '')
                        text = link.get_text(strip=True)
                        print(f"    {j+1}. {text} -> {href}")
                    break
                else:
                    print(f"  âš ï¸  ì£¼ë³´ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    test_church_website() 