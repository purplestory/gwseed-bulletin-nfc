#!/usr/bin/env python3
"""
ì •ê¸°ì ìœ¼ë¡œ ìµœì‹  ì£¼ë³´ë¥¼ í™•ì¸í•˜ê³  ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import requests
from bs4 import BeautifulSoup, Tag
import re
from datetime import datetime
import json
import os

def get_latest_bulletin_from_website():
    """êµíšŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì£¼ë³´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    
    # ì—¬ëŸ¬ User-Agent ì‹œë„
    user_agents = [
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
        'curl/7.68.0',  # GitHub Actions í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ curl
        # GitHub Actions í™˜ê²½ìš© ì¶”ê°€ User-Agent
        'Mozilla/5.0 (Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0',
        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        # ì¶”ê°€ ì‹¤ì œ ë¸Œë¼ìš°ì € User-Agent
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'
    ]
    
    for i, user_agent in enumerate(user_agents, 1):
        try:
            print(f"ğŸ”„ ì‹œë„ {i}/{len(user_agents)}: {user_agent[:50]}...")
            
            url = "https://www.godswillseed.or.kr/bbs/board.php?bo_table=weekly"
            
            # ë” ì‹¤ì œì ì¸ ë¸Œë¼ìš°ì € í—¤ë” ì„¤ì •
            headers = {
                'User-Agent': user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
                # GitHub Actions í™˜ê²½ìš© ì¶”ê°€ í—¤ë”
                'X-Forwarded-For': '127.0.0.1',
                'X-Real-IP': '127.0.0.1',
                'X-Requested-With': 'XMLHttpRequest',
                # ì¶”ê°€ ë´‡ ì°¨ë‹¨ ìš°íšŒ í—¤ë”
                'Referer': 'https://www.godswillseed.or.kr/',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-User': '?1'
            }
            
            # ì„¸ì…˜ ì‚¬ìš©ìœ¼ë¡œ ì¿ í‚¤ ìœ ì§€
            session = requests.Session()
            session.headers.update(headers)
            
            # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ì— ì ‘ì† (ë´‡ ê°ì§€ ë°©ì§€)
            print(f"ğŸ”„ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            main_response = session.get("https://www.godswillseed.or.kr/", timeout=15)
            main_response.raise_for_status()
            
            # ì ì‹œ ëŒ€ê¸° (ë´‡ ê°ì§€ ë°©ì§€)
            import time
            time.sleep(2)
            
            # JavaScript ì‹¤í–‰ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ì¶”ê°€ ìš”ì²­
            print(f"ğŸ”„ JavaScript í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜...")
            js_response = session.get("https://www.godswillseed.or.kr/cupid.js", timeout=10)
            time.sleep(1)
            
            # ì£¼ë³´ í˜ì´ì§€ ì ‘ì†
            print(f"ğŸ”„ ì£¼ë³´ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            response = session.get(url, timeout=15)
            response.raise_for_status()
            
            # ë””ë²„ê·¸: ì‘ë‹µ ë‚´ìš© í™•ì¸
            print(f"ğŸ“„ ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            print(f"ğŸ“„ ì‘ë‹µ í—¤ë”: {dict(response.headers)}")
            print(f"ğŸ“„ ì‘ë‹µ ë‚´ìš© ê¸¸ì´: {len(response.text)} ë¬¸ì")
            
            # ì‘ë‹µ ë‚´ìš©ì˜ ì¼ë¶€ë¥¼ ì¶œë ¥ (ë””ë²„ê·¸ìš©)
            content_preview = response.text[:500]
            print(f"ğŸ“„ ì‘ë‹µ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_preview}")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ìµœì‹  ì£¼ë³´ ë§í¬ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            latest_post = None
            
            # ë°©ë²• 1: ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ì—ì„œ wr_id íŒ¨í„´ ì°¾ê¸° (ê°€ì¥ ë¨¼ì € ì‹œë„)
            text_content = response.text
            wr_id_matches = re.findall(r'wr_id=(\d+)', text_content)
            if wr_id_matches:
                latest_wr_id = max(wr_id_matches, key=int)
                latest_url = f"https://www.godswillseed.or.kr/bbs/board.php?bo_table=weekly&wr_id={latest_wr_id}"
                
                latest_post = {
                    'url': latest_url,
                    'wr_id': latest_wr_id,
                    'title': f"ì£¼ë³´ {latest_wr_id}",
                    'timestamp': datetime.now().isoformat()
                }
                print(f"âœ… User-Agent {i}ë¡œ ì„±ê³µ (í…ìŠ¤íŠ¸ ê²€ìƒ‰): wr_id={latest_wr_id}")
                return latest_post
            
            # ë°©ë²• 2: ê¸°ì¡´ ë°©ë²• (tbody tr:not(.bo_notice))
            for post in soup.select("tbody tr:not(.bo_notice)"):
                link_tag = post.select_one("td.td_subject a")
                if link_tag and 'href' in link_tag.attrs:
                    href = str(link_tag['href'])
                    if 'wr_id=' in href and 'weekly' in href:
                        if not href.startswith('http'):
                            href = "https://www.godswillseed.or.kr" + href
                        
                        # wr_id ì¶”ì¶œ
                        wr_id_match = re.search(r'wr_id=(\d+)', href)
                        wr_id = wr_id_match.group(1) if wr_id_match else None
                        
                        # ì œëª© ì¶”ì¶œ
                        title = link_tag.get_text(strip=True)
                        
                        latest_post = {
                            'url': href,
                            'wr_id': wr_id,
                            'title': title,
                            'timestamp': datetime.now().isoformat()
                        }
                        print(f"âœ… User-Agent {i}ë¡œ ì„±ê³µ: wr_id={wr_id}")
                        return latest_post
            
            # ë°©ë²• 3: ëª¨ë“  ë§í¬ì—ì„œ wr_id ì°¾ê¸°
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = str(link['href'])
                if 'wr_id=' in href and 'weekly' in href:
                    if not href.startswith('http'):
                        href = "https://www.godswillseed.or.kr" + href
                    
                    # wr_id ì¶”ì¶œ
                    wr_id_match = re.search(r'wr_id=(\d+)', href)
                    wr_id = wr_id_match.group(1) if wr_id_match else None
                    
                    # ì œëª© ì¶”ì¶œ
                    title = link.get_text(strip=True)
                    
                    latest_post = {
                        'url': href,
                        'wr_id': wr_id,
                        'title': title,
                        'timestamp': datetime.now().isoformat()
                    }
                    print(f"âœ… User-Agent {i}ë¡œ ì„±ê³µ (ë°©ë²• 2): wr_id={wr_id}")
                    return latest_post
            
            print(f"âŒ User-Agent {i}ë¡œ ì‹¤íŒ¨: ì£¼ë³´ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ User-Agent {i}ë¡œ ì‹¤íŒ¨: {e}")
            continue
    
    print("âŒ ëª¨ë“  User-Agent ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼ì´ ì™„ì „íˆ ì‹¤íŒ¨í•œ ê²½ìš°
    print("âŒ êµíšŒ ì›¹ì‚¬ì´íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    return None

def get_latest_bulletin_from_file():
    """íŒŒì¼ì—ì„œ ìµœì‹  ì£¼ë³´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        if os.path.exists('latest_bulletin.json'):
            with open('latest_bulletin.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data
        return None
    except Exception as e:
        print(f"íŒŒì¼ì—ì„œ ìµœì‹  ì£¼ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return None

def update_latest_bulletin_file(bulletin_info):
    """ìµœì‹  ì£¼ë³´ ì •ë³´ë¥¼ íŒŒì¼ì— ì €ì¥"""
    try:
        with open('latest_bulletin.json', 'w', encoding='utf-8') as f:
            json.dump(bulletin_info, f, ensure_ascii=False, indent=2)
        print(f"âœ… ìµœì‹  ì£¼ë³´ ì •ë³´ íŒŒì¼ ì—…ë°ì´íŠ¸: {bulletin_info['title']}")
        return True
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def update_index_html(wr_id):
    """index.html íŒŒì¼ì—ì„œ wr_id ì—…ë°ì´íŠ¸"""
    try:
        with open('index.html', 'r', encoding='utf-8') as f:
            content = f.read()
        
        # wr_id íŒ¨í„´ ì°¾ê¸° ë° êµì²´ (ì—¬ëŸ¬ ê³³ì—ì„œ êµì²´)
        updated_content = re.sub(r'wr_id=\d+', f'wr_id={wr_id}', content)
        
        if content == updated_content:
            print("ğŸ”„ index.htmlì— ë³€ê²½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False # ë³€ê²½ì‚¬í•­ ì—†ìŒ

        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"âœ… index.html ì—…ë°ì´íŠ¸ ì™„ë£Œ: wr_id={wr_id}")
        return True
        
    except Exception as e:
        print(f"âŒ index.html ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def check_and_update_latest_bulletin():
    """ìµœì‹  ì£¼ë³´ í™•ì¸ ë° ì—…ë°ì´íŠ¸"""
    print("ğŸ” êµíšŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì£¼ë³´ í™•ì¸ ì¤‘...")
    
    # ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì£¼ë³´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    website_latest = get_latest_bulletin_from_website()
    if not website_latest:
        print("âŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì£¼ë³´ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    print(f"ğŸ“‹ ì›¹ì‚¬ì´íŠ¸ ìµœì‹  ì£¼ë³´: {website_latest['title']} (wr_id: {website_latest['wr_id']})")
    
    # íŒŒì¼ì—ì„œ ìµœì‹  ì£¼ë³´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    file_latest = get_latest_bulletin_from_file()
    if file_latest:
        print(f"ğŸ“‹ íŒŒì¼ ìµœì‹  ì£¼ë³´: {file_latest['title']} (wr_id: {file_latest['wr_id']})")
    else:
        print("ğŸ“‹ íŒŒì¼ì— ì£¼ë³´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìƒˆë¡œìš´ ì£¼ë³´ì¸ì§€ í™•ì¸
    if not file_latest or int(website_latest['wr_id']) > int(file_latest['wr_id']):
        print("ğŸ†• ìƒˆë¡œìš´ ì£¼ë³´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # index.html ì—…ë°ì´íŠ¸
        if update_index_html(website_latest['wr_id']):
            # ìµœì‹  ì£¼ë³´ ì •ë³´ íŒŒì¼ ì—…ë°ì´íŠ¸
            update_latest_bulletin_file(website_latest)
            
            print("ğŸ‰ ìƒˆë¡œìš´ ì£¼ë³´ê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ğŸ“ index.htmlì´ ìµœì‹  wr_idë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        else:
            print("âŒ index.html ì—…ë°ì´íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
    else:
        print("âœ… ì´ë¯¸ ìµœì‹  ì£¼ë³´ê°€ ìˆìŠµë‹ˆë‹¤.")
        # ìµœì‹  ì£¼ë³´ ì •ë³´ íŒŒì¼ ì—…ë°ì´íŠ¸
        update_latest_bulletin_file(website_latest)
        return True

if __name__ == "__main__":
    check_and_update_latest_bulletin() 